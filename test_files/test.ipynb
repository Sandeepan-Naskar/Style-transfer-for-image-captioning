{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to 'C:\\Users\\naska\\fiftyone\\coco-2017\\train' if necessary\n",
      "Found annotations at 'C:\\Users\\naska\\fiftyone\\coco-2017\\raw\\instances_train2017.json'\n",
      "Sufficient images already downloaded\n",
      "Existing download of split 'train' is sufficient\n",
      "Loading existing dataset 'coco-2017-train-1'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Load one sample from COCO dataset using FiftyOne\n",
    "dataset = foz.load_zoo_dataset(\"coco-2017\", split=\"train\", max_samples=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first sample from the dataset\n",
    "sample = dataset.first()\n",
    "\n",
    "# Load the image using its filepath\n",
    "image_path = sample[\"filepath\"]\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Display the original image\n",
    "image.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65f870077a3b3e2da43709d7\n",
      "['bowl', 'bowl', 'broccoli', 'bowl', 'orange', 'orange', 'orange', 'orange']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the image\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Get the image embeddings\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "\n",
    "# Load COCO captions for the corresponding image\n",
    "image_id = dataset.match({\"filepath\": image_path}).first().id\n",
    "\n",
    "print(image_id)\n",
    "list_of_detections = sample.ground_truth.detections\n",
    "\n",
    "labels = []\n",
    "for detection in list_of_detections:\n",
    "    labels.append(detection.label)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stylized_image = image_input.clone()\n",
    "# Set optimization parameters\n",
    "optimizer = torch.optim.Adam([stylized_image], lr=0.1)\n",
    "\n",
    "# # Perform style transfer (for demonstration, let's just use random noise)\n",
    "stylized_image_features = image_features + 0.1 * torch.randn_like(image_features)\n",
    "\n",
    "# Optimization loop\n",
    "for _ in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get CLIP features for the stylized image\n",
    "    stylized_image_features = model.encode_image(stylized_image)\n",
    "    \n",
    "    # Loss function: minimize the distance between stylized image features and combined features\n",
    "    loss = torch.nn.functional.mse_loss(stylized_image_features, image_features + 0.1 * torch.randn_like(image_features)\n",
    ")\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# # Convert the stylized image tensor to a PIL image\n",
    "# stylized_image = transforms.functional.to_pil_image(stylized_image.squeeze().cpu())\n",
    "\n",
    "# Convert the stylized image tensor to a numpy array\n",
    "stylized_image_np = stylized_image[0].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# Display the stylized image\n",
    "Image.fromarray((stylized_image_np * 255).astype(np.uint8)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = labels\n",
    "\n",
    "# Choose a random caption\n",
    "random_caption = \"green forest\"  # Just choose the first caption for simplicity\n",
    "\n",
    "# Preprocess the text\n",
    "text_input = clip.tokenize([random_caption]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the text embeddings\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_input)\n",
    "\n",
    "# stylized_image_features = image_features + text_features\n",
    "\n",
    "# Initialize the stylized image as the original image\n",
    "stylized_image = image_input.clone().requires_grad_()\n",
    "\n",
    "# Set optimization parameters\n",
    "optimizer = torch.optim.Adam([stylized_image], lr=0.1)\n",
    "\n",
    "# Optimization loop\n",
    "for _ in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get CLIP features for the stylized image\n",
    "    stylized_image_features = model.encode_image(stylized_image)\n",
    "    \n",
    "    # Loss function: minimize the distance between stylized image features and combined features\n",
    "    loss = torch.nn.functional.mse_loss(stylized_image_features, image_features + text_features)\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Convert the stylized image tensor to a PIL image\n",
    "stylized_image = transforms.functional.to_pil_image(stylized_image.squeeze().cpu())\n",
    "\n",
    "# Display the stylized image\n",
    "stylized_image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
